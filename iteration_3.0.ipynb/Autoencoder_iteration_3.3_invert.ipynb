{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder_iteration_3.0",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyO+X0JEFMjSkH42yVpzhffj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/msc-ai-project/blob/main/iteration_3.0.ipynb/Autoencoder_iteration_3.3_invert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc_scores = {}\n",
        "N_EPOCHS = 2\n",
        "INVERT_TRAIN_TEST = False # If False, train with encrypted and test with non-encrypted\n",
        "INVERT_TRAIN_TEST = True # If False, train with encrypted and test with non-encrypted\n",
        "LOSS_FUNCTION='mse'   # mse or binary_crossentropy\n",
        "\n",
        "RANDOM_SEED = 42"
      ],
      "metadata": {
        "id": "r0itFEsTUQ4r"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0crS396ZON0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598e6dd0-bbaa-4ffe-8b14-74f75c40ed6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging, sys, random, glob\n",
        "from google.colab import drive\n",
        "from functools import lru_cache\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "import IPython\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "def set_random_seed():\n",
        "    np.random.seed(1)\n",
        "    random.seed(1)\n",
        "    tf.random.set_seed(1)\n",
        "\n",
        "root = logging.getLogger()\n",
        "root.setLevel(logging.INFO)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setLevel(logging.DEBUG)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "root.addHandler(handler)\n",
        "\n",
        "!if [[ -d /content/drive/MyDrive/MSCPROJDATA ]]; then cp -r /content/drive/MyDrive/MSCPROJDATA .; fi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_columns(thisdf):\n",
        "    baseline_columns = [c for c in thisdf.columns if c.startswith('baseline') and \"head\" not in c and \"tail\" not in c]\n",
        "    baseline_columns = [c for c in baseline_columns if \"filesize\" not in c]\n",
        "    baseline_columns = [c for c in baseline_columns if \"begin\" not in c and \"end\" not in c]\n",
        "\n",
        "    advanced_columns = [c for c in thisdf.columns if \"advanced\" in c]\n",
        "    advanced_columns = [c for c in advanced_columns if \"begin\" not in c and \"end\" not in c]\n",
        "    advanced_columns = [c for c in advanced_columns if \"head\" not in c and \"tail\" not in c]\n",
        "    advanced_columns = [c for c in advanced_columns if \"start\" not in c]\n",
        "    advanced_columns_only = list(set(advanced_columns))\n",
        "    advanced_columns = list(set(advanced_columns + baseline_columns))\n",
        "\n",
        "    fourier_columns = [c for c in thisdf.columns if \"fourier\" in c and \"value\" not in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"1byte\" in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"begin\" not in c and \"end\" not in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"head\" not in c and \"tail\" not in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"start\" not in c]\n",
        "    fourier_columns_only = list(set(fourier_columns))\n",
        "    fourier_columns = list(set(advanced_columns + fourier_columns))\n",
        "    \n",
        "    baseline_and_advanced = list(set(baseline_columns + advanced_columns_only))\n",
        "    baseline_and_fourier = list(set(baseline_columns + fourier_columns_only))\n",
        "    advanced_and_fourier = list(set(advanced_columns_only + fourier_columns_only))\n",
        "\n",
        "    all_columns = [c for c in thisdf.columns]\n",
        "    \n",
        "    return {\\\n",
        "        \"baseline\": baseline_columns,\\\n",
        "        \"advanced-only\": advanced_columns_only,\\\n",
        "        \"fourier-only\": fourier_columns_only,\\\n",
        "        \"baseline-and-fourier\": baseline_and_fourier,\\\n",
        "        \"advanced-and-fourier\": advanced_and_fourier,\\\n",
        "        \"advanced\": advanced_columns,\\\n",
        "        \"fourier\": fourier_columns,\\\n",
        "        \"all_columns\": all_columns,\n",
        "    }\n",
        "\n",
        "# @lru_cache(maxsize=5)\n",
        "def load_datasets_once():\n",
        "    \"\"\"Load all datasets only once\n",
        "    \n",
        "    We want to load the datasets only once. Once loaded\n",
        "    serve from cache\n",
        "    \"\"\"\n",
        "    datasets = dict()\n",
        "    for file in glob.glob(\"MSCPROJDATA/**.parquet.gz\", recursive=True):\n",
        "        if not file.startswith(\"MSCPROJDATA/n1\"):\n",
        "            print(f\"Loading {file}\")\n",
        "            df = pd.read_parquet(file)\n",
        "            df = df.sample(frac=1).reset_index(drop=True)\n",
        "            df[\"is_encrypted\"] = 1 if \"encr\" in file.lower() else 0\n",
        "            datasets[file] = df\n",
        "    return datasets\n",
        "    \n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "uZn6_lXPTsAK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_all_dfs = load_datasets_once()\n",
        "temp_all_dfs['MSCPROJDATA/plaintext.base32.combined.parquet.gz'].columns\n",
        "\n",
        "all_column_names = get_columns(temp_all_dfs['MSCPROJDATA/plaintext.base32.combined.parquet.gz'])\n",
        "print(all_column_names.keys())\n",
        "temp_all_dfs = None\n",
        "\n",
        "import gc\n",
        "def gc_collect():\n",
        "    [gc.collect(i) for i in range(3) for j in range(3)]\n",
        "\n",
        "gc_collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dLxX6Z--bY9",
        "outputId": "e7086f1b-c3b2-418e-b571-6aaf763e31c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MSCPROJDATA/plaintext.base32.combined.parquet.gz\n",
            "Loading MSCPROJDATA/plaintext.expanded.parquet.gz\n",
            "Loading MSCPROJDATA/expanded.plaintext.base32.parquet.gz\n",
            "Loading MSCPROJDATA/plaintext.combined.parquet.gz\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v1.b32.parquet.gz\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v2.base32.parquet.gz\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v1.parquet.gz\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v2.parquet.gz\n",
            "dict_keys(['baseline', 'advanced-only', 'fourier-only', 'baseline-and-fourier', 'advanced-and-fourier', 'advanced', 'fourier', 'all_columns'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_column_names.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fJXnsBHBJwd",
        "outputId": "88630f66-9705-4f38-d0f3-0e618e4fa2c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['baseline', 'advanced-only', 'fourier-only', 'baseline-and-fourier', 'advanced-and-fourier', 'advanced', 'fourier', 'all_columns'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_datasets = load_datasets_once()\n",
        "full_dataset = pd.concat(all_datasets.values())\n",
        "full_dataset = full_dataset.fillna(0.0)\n",
        "\n",
        "if not INVERT_TRAIN_TEST:\n",
        "    encrypted_df = full_dataset[full_dataset[\"is_encrypted\"] == 1]\n",
        "    non_encrypted_df = full_dataset[full_dataset[\"is_encrypted\"] == 0]\n",
        "else:\n",
        "    encrypted_df = full_dataset[full_dataset[\"is_encrypted\"] == 0]\n",
        "    non_encrypted_df = full_dataset[full_dataset[\"is_encrypted\"] == 1]\n",
        "\n",
        "discard_some = True\n",
        "if discard_some:\n",
        "    retain = full_dataset['extended.base_filename'].map(lambda x: \".webp\" not in x and \"b32\" not in x and \"base32\" not in x)\n",
        "    full_dataset = full_dataset[retain]\n",
        "\n",
        "# shuffle\n",
        "encrypted_df = encrypted_df.sample(frac=1).reset_index(drop=True)\n",
        "non_encrypted_df = non_encrypted_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "columns = all_column_names[\"all_columns\"]\n",
        "print(f\"Number of features in full dataset: {len(columns)}\")\n",
        "\n",
        "work_encrypted_df_X = encrypted_df[columns]\n",
        "work_encrypted_df_y = encrypted_df[\"is_encrypted\"]\n",
        "work_non_encrypted_df_X = non_encrypted_df[columns]\n",
        "work_non_encrypted_df_y = non_encrypted_df[\"is_encrypted\"]\n",
        "\n",
        "\n",
        "columns = []\n",
        "for i, j in zip(work_encrypted_df_X.dtypes, work_encrypted_df_X.columns):\n",
        "    if \"float\" in str(i):\n",
        "        columns.append(j)\n",
        "\n",
        "work_encrypted_df_X = work_encrypted_df_X[columns]\n",
        "work_non_encrypted_df_X = work_non_encrypted_df_X[columns]\n",
        "\n",
        "import gc\n",
        "del all_datasets\n",
        "del encrypted_df\n",
        "del non_encrypted_df\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        gc.collect(j)\n",
        "        gc.collect(i)\n",
        "\n",
        "def normalize_df(X, Y):\n",
        "    mindf = X.min()\n",
        "    maxdf = X.max()\n",
        "    print(mindf.shape)\n",
        "    X = (X - mindf) / (maxdf - mindf)\n",
        "    Y = (Y - mindf) / (maxdf - mindf)\n",
        "    return X, Y\n",
        "\n",
        "work_encrypted_df_X, work_non_encrypted_df_X = \\\n",
        "    normalize_df(\n",
        "        work_encrypted_df_X,\n",
        "        work_non_encrypted_df_X\n",
        "    )\n",
        "\n",
        "gc_collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUYpSq8jUK0p",
        "outputId": "1b999b1c-ed3f-43d7-c465-fd133b52b0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MSCPROJDATA/plaintext.base32.combined.parquet.gz\n",
            "Loading MSCPROJDATA/plaintext.expanded.parquet.gz\n",
            "Loading MSCPROJDATA/expanded.plaintext.base32.parquet.gz\n",
            "Loading MSCPROJDATA/plaintext.combined.parquet.gz\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v1.b32.parquet.gz\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v2.base32.parquet.gz\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v1.parquet.gz\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v2.parquet.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(\\\n",
        "        work_encrypted_df_X,\n",
        "        work_encrypted_df_y,\n",
        "        test_size=0.33,\n",
        "        random_state=42)\n",
        "\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "Gade9112UWAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_and_plot(model):\n",
        "    print(model.summary())\n",
        "    plot = tf.keras.utils.plot_model(\n",
        "        model,\n",
        "        show_shapes=True,\n",
        "        expand_nested=True)\n",
        "    IPython.display.display(plot)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "def create_and_train_autoencoder(X, y, model, n_epochs):\n",
        "    X_train, X_eval, y_train, y_eval= \\\n",
        "        train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    nn = model\n",
        "    nn.compile(optimizer='adam', loss=LOSS_FUNCTION, metrics=[\"mae\", \"mse\"])\n",
        "\n",
        "    nn.build((None, X.shape[1]))\n",
        "    print_and_plot(nn)\n",
        "\n",
        "    print(X_train.shape, X_eval.shape)\n",
        "    history = nn.fit(X_train, X_train,\n",
        "                epochs=n_epochs,\n",
        "                validation_data=(X_eval, X_eval),\n",
        "                batch_size=32)\n",
        "    return nn, history\n",
        "\n",
        "\n",
        "def evaluate_model(model, columns:list, n_epochs:int, description:str):\n",
        "    tf.random.set_seed(RANDOM_SEED)\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    c1 = set([c for c in work_encrypted_df_X.columns])\n",
        "    c2 = set(columns)\n",
        "    columns = c1.intersection(c2)\n",
        "    print(f\"Number of features: {len(columns)}\")\n",
        "\n",
        "    # Encrypted section, use half for training, we will use the remaining\n",
        "    # half for testing\n",
        "    temp_work_encrypted_df_X = work_encrypted_df_X[columns]\n",
        "    temp_work_non_encrypted_df_X = work_non_encrypted_df_X[columns]\n",
        "\n",
        "    nn = model\n",
        "\n",
        "    columns = []\n",
        "    for i, j in zip(temp_work_encrypted_df_X.dtypes, temp_work_encrypted_df_X.columns):\n",
        "        if \"float\" in str(i):\n",
        "            columns.append(j)\n",
        "    temp_work_encrypted_df_X = temp_work_encrypted_df_X[columns]\n",
        "    temp_work_non_encrypted_df_X = temp_work_non_encrypted_df_X[columns]\n",
        "\n",
        "    eTrain_X, eTest_X, eTrain_y, eTest_y = \\\n",
        "        train_test_split(\\\n",
        "                        temp_work_encrypted_df_X,\\\n",
        "                        work_encrypted_df_y,\\\n",
        "                        test_size=0.2,\\\n",
        "                        random_state=42)\n",
        "\n",
        "    nn, history = create_and_train_autoencoder(eTrain_X, eTrain_y, model, n_epochs)\n",
        "\n",
        "    _, neTest_X, _, neTest_y = \\\n",
        "        train_test_split(\\\n",
        "                        temp_work_non_encrypted_df_X,\\\n",
        "                        work_non_encrypted_df_y,\\\n",
        "                        test_size=0.2,\\\n",
        "                        random_state=42)\n",
        "\n",
        "    final_test_X = pd.concat([eTest_X, neTest_X])\n",
        "    final_test_y = pd.concat([eTest_y, neTest_y])\n",
        "\n",
        "    # ---------------------\n",
        "    plt.rcParams[\"figure.figsize\"] = (6, 3)\n",
        "    \n",
        "    plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "    for metric in [\"loss\", \"mse\", \"mae\"]:\n",
        "        keys = []\n",
        "        for k, v in history.history.items():\n",
        "            if metric in k.lower():\n",
        "                keys.append(k)\n",
        "                plt.plot(v, label=k)\n",
        "            plt.legend(keys)\n",
        "        plt.show()\n",
        "\n",
        "    # ----------------------------\n",
        "\n",
        "    plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "    def get_mse_mae(X, X1):\n",
        "        saveX, saveX1 = X, X1\n",
        "        if not isinstance(X, np.ndarray):\n",
        "            X = X.to_numpy()\n",
        "        if not isinstance(X1, np.ndarray):\n",
        "            X1 = X1.to_numpy()\n",
        "        square_error = np.square(X - X1)\n",
        "        mse = np.mean(square_error, axis=1)\n",
        "        abs_error = np.square(X - X1)\n",
        "        mae = np.mean(abs_error, axis=1)\n",
        "        return mse, mae\n",
        "    \n",
        "    \n",
        "    \n",
        "    reconstructed_test_X = nn.predict(final_test_X)\n",
        "    mse, mae = get_mse_mae(final_test_X, reconstructed_test_X)\n",
        "    \n",
        "    \n",
        "    df_result = pd.DataFrame(\\\n",
        "        {\\\n",
        "            \"is_encrypted\": final_test_y,\\\n",
        "            \"mse\": mse,\\\n",
        "            \"mae\": mae,\\\n",
        "        }\\\n",
        "    )\n",
        "    \n",
        "    #df_result2 = df_result[df_result[\"mae\"] < 0.1]\n",
        "\n",
        "    def show_strip_plot(df_result):\n",
        "        if not INVERT_TRAIN_TEST:\n",
        "            x_text = \"is_encrypted\"\n",
        "        else:\n",
        "            x_text = \"is_plaintext\"\n",
        "        df_result2 = df_result.copy()\n",
        "        if INVERT_TRAIN_TEST:\n",
        "            df_result2.rename(columns={\"is_encrypted\": \"is_plaintext\"}, inplace=True)\n",
        "        sns.stripplot(data=df_result2, x=x_text, y=\"mse\", jitter=0.5, size=0.7).set(title=\"Mean Square Error\")\n",
        "        plt.gca().set_yscale('log')\n",
        "        plt.show()\n",
        "        sns.boxplot(data=df_result2, x=x_text, y=\"mse\").set(title=\"Mean Square Error\")\n",
        "        \n",
        "        plt.gca().set_yscale('log')\n",
        "        plt.show()\n",
        "    \n",
        "    df = df_result\n",
        "    df = df.sort_values(by=\"mse\").reset_index()\n",
        "    y_pred = np.log(df[\"mse\"].to_numpy())\n",
        "    y_true = df[\"is_encrypted\"].to_numpy()\n",
        "    \n",
        "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_true=y_true, y_score=y_pred)\n",
        "    auc_score = auc(fpr, tpr)\n",
        "    print()\n",
        "    print()\n",
        "    plt.plot(fpr, tpr)\n",
        "    print()\n",
        "    print()\n",
        "    print(f\"AUC Score for {description} is : {auc_score}\")\n",
        "\n",
        "    return description, auc_score"
      ],
      "metadata": {
        "id": "qjb40EnGabED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_AllColumns:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = tf.keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(668, activation='relu'),\n",
        "                layers.Dropout(0.25),\n",
        "                layers.Dense(668, activation='relu'),\n",
        "                layers.Dropout(0.25),\n",
        "                layers.Dense(512, activation='relu'),\n",
        "                layers.Dropout(0.1),\n",
        "                layers.Dense(256, activation='relu'),\n",
        "                layers.Dropout(0.1),\n",
        "                layers.Dense(128, activation='relu'),\n",
        "                layers.Dropout(0.05),\n",
        "                layers.Dense(64, activation='relu'),\n",
        "                layers.Dense(32, activation='relu'),\n",
        "                layers.Dense(64, activation='relu'),\n",
        "                layers.Dense(128, activation='relu'),\n",
        "                layers.Dense(256, activation='relu'),\n",
        "                layers.Dense(512, activation='relu'),\n",
        "                layers.Dense(668, activation='relu'),\n",
        "                layers.Dense(668, activation='relu'),\n",
        "            ]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "\n",
        "description, score = evaluate_model(Autoencoder_AllColumns.build(), all_column_names['all_columns'], N_EPOCHS, \"All columns\")\n",
        "auc_scores[description] = score"
      ],
      "metadata": {
        "id": "nA8s1dxgC_Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_baseline:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = tf.keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(4, activation='relu'),\n",
        "                layers.Dense(3, activation='relu'),\n",
        "                layers.Dense(2, activation='relu'),\n",
        "                layers.Dense(3, activation='relu'),\n",
        "                layers.Dense(4, activation='relu'),\n",
        "                layers.Dense(4, activation='sigmoid'),\n",
        "            ]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "\n",
        "description, score = evaluate_model(Autoencoder_baseline.build(), all_column_names['baseline'], N_EPOCHS, \"baseline\")\n",
        "auc_scores[description] = score"
      ],
      "metadata": {
        "id": "NQCa6O9FIMaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_AdvancedOnly:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = tf.keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(40, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(30, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(15, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(7, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(15, activation='relu'),\n",
        "                layers.Dense(30, activation='relu'),\n",
        "                layers.Dense(40, activation='relu'),\n",
        "                layers.Dense(40, activation='sigmoid'),\n",
        "            ]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "\n",
        "gc_collect()\n",
        "name = 'advanced-only'\n",
        "description, score = evaluate_model(Autoencoder_AdvancedOnly.build(), all_column_names[name], N_EPOCHS, name)\n",
        "auc_scores[description] = score"
      ],
      "metadata": {
        "id": "ru3CLL5HLnIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_FourierOnly:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = tf.keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(18, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(12, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(8, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(7, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(8, activation='relu'),\n",
        "                layers.Dense(12, activation='relu'),\n",
        "                layers.Dense(18, activation='relu'),\n",
        "                layers.Dense(18, activation='sigmoid'),\n",
        "            ]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "gc_collect()\n",
        "name = 'fourier-only'\n",
        "description, score = evaluate_model(Autoencoder_FourierOnly.build(), all_column_names[name], N_EPOCHS, name)\n",
        "auc_scores[description] = score"
      ],
      "metadata": {
        "id": "QMXQ94-0MpzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc_collect()"
      ],
      "metadata": {
        "id": "BaG8llhvNDiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_BaselineAndAdvanced:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = tf.keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(44, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(40, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(30, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(15, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(7, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(15, activation='relu'),\n",
        "                layers.Dense(30, activation='relu'),\n",
        "                layers.Dense(40, activation='relu'),\n",
        "                layers.Dense(44, activation='relu'),\n",
        "                layers.Dense(44, activation='sigmoid'),\n",
        "            ]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "gc_collect()\n",
        "name = 'advanced'\n",
        "description, score = evaluate_model(Autoencoder_BaselineAndAdvanced.build(), all_column_names[name], N_EPOCHS, \"baseline-and-advanced\")\n",
        "auc_scores[description] = score"
      ],
      "metadata": {
        "id": "xXn5o8ccOtJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_column_names.keys()"
      ],
      "metadata": {
        "id": "Of_JvHSIODj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_BaselineAndFourier:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = tf.keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(22, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(15, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(10, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(7, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(10, activation='relu'),\n",
        "                layers.Dense(15, activation='relu'),\n",
        "                layers.Dense(22, activation='relu'),\n",
        "                layers.Dense(22, activation='sigmoid'),\n",
        "            ]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "gc_collect()\n",
        "name = 'baseline-and-fourier'\n",
        "description, score = evaluate_model(Autoencoder_BaselineAndFourier.build(), all_column_names[name], N_EPOCHS, name)\n",
        "auc_scores[description] = score"
      ],
      "metadata": {
        "id": "XyWtu8i4P_B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_AdvancedAndFourier:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = tf.keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(58, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(50, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(25, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(12, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(8, activation='relu'),\n",
        "                layers.Dense(12, activation='relu'),\n",
        "                layers.Dense(25, activation='relu'),\n",
        "                layers.Dense(50, activation='relu'),\n",
        "                layers.Dense(58, activation='relu'),\n",
        "                layers.Dense(58, activation='sigmoid'),\n",
        "            ]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "gc_collect()\n",
        "name = 'advanced-and-fourier'\n",
        "description, score = evaluate_model(Autoencoder_AdvancedAndFourier.build(), all_column_names[name], N_EPOCHS, name)\n",
        "auc_scores[description] = score"
      ],
      "metadata": {
        "id": "nyiD7rk0Qx2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_BaselineAdvancedAndFourier:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = tf.keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(62, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(55, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(50, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(25, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(12, activation='relu'),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(8, activation='relu'),\n",
        "                layers.Dense(12, activation='relu'),\n",
        "                layers.Dense(25, activation='relu'),\n",
        "                layers.Dense(50, activation='relu'),\n",
        "                layers.Dense(55, activation='relu'),\n",
        "                layers.Dense(62, activation='relu'),\n",
        "                layers.Dense(62, activation='sigmoid'),\n",
        "            ]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "gc_collect()\n",
        "name = 'fourier'\n",
        "description, score = evaluate_model(Autoencoder_BaselineAdvancedAndFourier.build(), all_column_names[name], N_EPOCHS, \"baseline-advanced-and-fourier\")\n",
        "auc_scores[description] = score"
      ],
      "metadata": {
        "id": "hqqiMaoFRlpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(auc_scores)"
      ],
      "metadata": {
        "id": "nSGECx2yRgBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in auc_scores.items():\n",
        "    print(k, v)"
      ],
      "metadata": {
        "id": "kq3q7uu7UEJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = [k for k in auc_scores.keys()]\n",
        "auc = [a for a in auc_scores.values()]\n",
        "\n",
        "def rename_feature(x):\n",
        "    out = \"\"\n",
        "    last_char = None\n",
        "    for c in x:\n",
        "        if last_char is None:\n",
        "            c = c.upper()\n",
        "            last_char = c\n",
        "        elif last_char == '-':\n",
        "            c = c.upper()\n",
        "            last_char = c\n",
        "        elif c == '-':\n",
        "            last_char = c\n",
        "            c = ' '\n",
        "        elif last_char == ' ':\n",
        "            c = c.upper()\n",
        "            last_char = c\n",
        "        else:\n",
        "            last_char = c\n",
        "        out += c\n",
        "    return out\n",
        "        \n",
        "\n",
        "df = pd.DataFrame({\"Feature Set\": keys, \"AUC\": auc}).sort_values(by=\"AUC\", ascending=False).reset_index(drop=True)\n",
        "df[\"Feature Set\"] = df[\"Feature Set\"].map(rename_feature)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "9AKSmtMaUfZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.to_latex())"
      ],
      "metadata": {
        "id": "ypwizREGVoel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tT-ULFGoVsSd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}