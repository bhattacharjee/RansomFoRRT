{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-ann-v1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6xwwN4e3CUzBgkR3gOgQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/msc-ai-project/blob/main/DeepANN/deep_ann_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQP4gxMADtVr",
        "outputId": "12a8c4af-cbad-4cc8-84dd-5e057c32ce8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "USE_FLOAT32 = False\n",
        "REDUCE_DF_SIZE = True\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "INCLUDE_NAPIERONE = True\n",
        "\n",
        "!pip install tensorflow-addons\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging, sys, random, glob\n",
        "from google.colab import drive\n",
        "from functools import lru_cache\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from tensorflow import keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "import IPython\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import gc\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "def set_random_seed():\n",
        "    np.random.seed(1)\n",
        "    random.seed(1)\n",
        "    tf.random.set_seed(1)\n",
        "\n",
        "root = logging.getLogger()\n",
        "root.setLevel(logging.INFO)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setLevel(logging.DEBUG)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "root.addHandler(handler)\n",
        "\n",
        "!if [[ -d /content/drive/MyDrive/MSCPROJDATA && ! -d MSCPROJDATA ]]; then cp -r /content/drive/MyDrive/MSCPROJDATA .; fi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " def plot_history(history):\n",
        "    import matplotlib.pyplot as plt\n",
        "    keys = [str(k) for k in history.history.keys()]\n",
        "    keys = keys[0:(len(keys)//2)]\n",
        "\n",
        "    length = 10\n",
        "    height = length * 0.5 * len(keys)\n",
        "    plt.rcParams[\"figure.figsize\"] = (length, height)\n",
        "    plt.rcParams['font.size'] = 15\n",
        "    \n",
        "    fig, ax = plt.subplots(len(keys))\n",
        "\n",
        "\n",
        "    for i in range(len(keys)):\n",
        "        key = keys[i]\n",
        "        val_key = f\"val_{key}\"\n",
        "        ax[i].plot(history.history[key], label=key)\n",
        "        ax[i].plot(history.history[val_key], label=val_key)\n",
        "        ax[i].legend()\n",
        "\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iX5HQN0vXzC7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_names = [\"baseline\", \"advanced-only\", \"fourier-only\", \"baseline-and-fourier\", \"advanced-and-fourier\", \"advanced\", \"fourier\"]\n",
        "def get_columns_from_df(thisdf):\n",
        "    baseline_columns = [c for c in thisdf.columns if c.startswith('baseline') and \"head\" not in c and \"tail\" not in c]\n",
        "    baseline_columns = [c for c in baseline_columns if \"filesize\" not in c]\n",
        "    baseline_columns = [c for c in baseline_columns if \"begin\" not in c and \"end\" not in c]\n",
        "\n",
        "    advanced_columns = [c for c in thisdf.columns if \"advanced\" in c]\n",
        "    advanced_columns = [c for c in advanced_columns if \"begin\" not in c and \"end\" not in c]\n",
        "    advanced_columns = [c for c in advanced_columns if \"head\" not in c and \"tail\" not in c]\n",
        "    advanced_columns = [c for c in advanced_columns if \"start\" not in c]\n",
        "    advanced_columns_only = list(set(advanced_columns))\n",
        "    advanced_columns = list(set(advanced_columns + baseline_columns))\n",
        "\n",
        "    fourier_columns = [c for c in thisdf.columns if \"fourier\" in c and \"value\" not in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"1byte\" in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"begin\" not in c and \"end\" not in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"head\" not in c and \"tail\" not in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"start\" not in c]\n",
        "    fourier_columns_only = list(set(fourier_columns))\n",
        "    fourier_columns = list(set(advanced_columns + fourier_columns))\n",
        "    \n",
        "    baseline_and_advanced = list(set(baseline_columns + advanced_columns_only))\n",
        "    baseline_and_fourier = list(set(baseline_columns + fourier_columns_only))\n",
        "    advanced_and_fourier = list(set(advanced_columns_only + fourier_columns_only))\n",
        "\n",
        "    all_columns = [c for c in thisdf.columns]\n",
        "    \n",
        "    return {\\\n",
        "        \"all_columns\": all_columns,\n",
        "        \"Baseline\": baseline_columns,\\\n",
        "        \"Advanced Only\": advanced_columns_only,\\\n",
        "        \"Fourier Only\": fourier_columns_only,\\\n",
        "        \"Baseline and Advanced\": advanced_columns,\\\n",
        "        \"Baseline and Fourier\": baseline_and_fourier,\\\n",
        "        \"Advanced and Fourier\": advanced_and_fourier,\\\n",
        "        \"Baseline, Advanced and Fourier\": fourier_columns,\\\n",
        "    }\n",
        "\n",
        "\n",
        "def reduce_df_size(df):\n",
        "    original_usage = df.memory_usage().sum() / (1024**2)\n",
        "    for col in df.columns:\n",
        "        if \"fourier.value\" in col or \\\n",
        "            \"tail\" in col or \\\n",
        "            \"head\" in col or \\\n",
        "            \"end\" in col or \\\n",
        "            \"begin\" in col:\n",
        "            df.drop([col], axis=1, inplace=True)\n",
        "            continue\n",
        "        if USE_FLOAT32:\n",
        "            if \"float64\" in str(df[col].dtype):\n",
        "                df[col] = df[col].astype('float32')\n",
        "        elif \"int64\" in str(df[col].dtype):\n",
        "            df[col] = df[col].astype(\"int8\")\n",
        "    new_usage = df.memory_usage().sum() / (1024**2)\n",
        "    return df\n",
        "\n",
        "def call_gc():\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            gc.collect(j)\n",
        "\n",
        "@lru_cache(maxsize=5)\n",
        "def load_datasets():\n",
        "    global INCLUDE_NAPIERONE\n",
        "    datasets = dict()\n",
        "    print(\"Reloading Datasets\")\n",
        "    for file in glob.glob(\"MSCPROJDATA/**.parquet.gz\", recursive=True):\n",
        "        if INCLUDE_NAPIERONE or not file.startswith(\"MSCPROJDATA/n1\"):\n",
        "            df = pd.read_parquet(file)\n",
        "            datasets[file] = df\n",
        "            df[\"is_encrypted\"] = 1 if \"encr\" in file.lower() else 0\n",
        "    \n",
        "            if REDUCE_DF_SIZE:\n",
        "                df = reduce_df_size(df)\n",
        "\n",
        "            df = df.sample(frac=1).reset_index(drop=True)\n",
        "            datasets[file] = df\n",
        "            call_gc()\n",
        "\n",
        "    return datasets\n",
        "\n",
        "@lru_cache(maxsize=5)\n",
        "def get_columns():\n",
        "    datasets = load_datasets()\n",
        "    df = list(datasets.values())[0]\n",
        "    return get_columns_from_df(df)\n"
      ],
      "metadata": {
        "id": "UxRiyNG5EAiU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(maxsize=5)\n",
        "def get_train_test_df():\n",
        "    datasets = load_datasets()\n",
        "    df = pd.concat([v for v in datasets.values()])\n",
        "    x_cols = [str(c) for c in df.columns if \"is_encrypted\" != str(c)]\n",
        "    y_cols = \"is_encrypted\"\n",
        "\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    df.fillna(0.0)\n",
        "\n",
        "    X = df[x_cols]\n",
        "    y = df[y_cols]\n",
        "\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    X = X - X.min()\n",
        "    X = X / X.max()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "pMTF4z0nRNxM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    @staticmethod\n",
        "    def print_and_plot(model):\n",
        "        print(model.summary())\n",
        "        plot = tf.keras.utils.plot_model(\n",
        "            model,\n",
        "            show_shapes=True,\n",
        "            expand_nested=True)\n",
        "        IPython.display.display(plot)\n",
        "\n",
        "    @staticmethod\n",
        "    def create(name:str, columns:list):\n",
        "        X_train, X_test, y_train, y_test = get_train_test_df()\n",
        "        X_train = X_train[columns].to_numpy()\n",
        "\n",
        "        model = tf.keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(8, input_dim=X_train.shape[1], activation='relu', name=\"Dense-8\"),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(4, activation = 'relu', name=\"Dense-4\"),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(2, activation = 'relu', name=\"Dense-2\"),\n",
        "                layers.Dense(1, activation = 'sigmoid')\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        model.compile(\\\n",
        "            loss = 'binary_crossentropy',\n",
        "            optimizer = 'adam',\\\n",
        "            metrics = [\n",
        "                tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "                tf.keras.metrics.Precision(name='precision'),\n",
        "                tf.keras.metrics.Recall(name='recall'),\n",
        "                tfa.metrics.F1Score(num_classes=1, name='f1_score'),\n",
        "            ])\n",
        "            #callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")])\n",
        "        \n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def train_model(name):\n",
        "        tf.random.set_seed(42)\n",
        "        np.random.seed(42)\n",
        "        X_train, X_test, y_train, y_test = get_train_test_df()\n",
        "\n",
        "        columns = get_columns()\n",
        "        columns = columns[name]\n",
        "\n",
        "        X_train = X_train[columns]\n",
        "        X_test = X_test[columns]\n",
        "\n",
        "        n_cross_val = X_train.shape[0]\n",
        "        n_train = n_cross_val - n_cross_val // 5\n",
        "        n_cross_val = n_cross_val // 5\n",
        "\n",
        "        X_cross_val = X_train.tail(n_cross_val)\n",
        "        y_cross_val = y_train[-n_cross_val:]\n",
        "\n",
        "        X_train = X_train.head(n_train)\n",
        "        y_train = y_train[:n_train]\n",
        "\n",
        "        es_clbk = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
        "\n",
        "        model = Model.create(name, columns)\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            epochs=NUM_EPOCHS,\n",
        "                            validation_data=(X_cross_val, y_cross_val),\n",
        "                            callbacks=[es_clbk],\n",
        "                            batch_size=BATCH_SIZE)\n",
        "\n",
        "        Model.print_and_plot(model)\n",
        "        \n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_true=y_test, y_score=y_pred)\n",
        "        auc = sklearn.metrics.auc(fpr, tpr)\n",
        "        y_pred = y_pred > 0.5\n",
        "\n",
        "        acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
        "        prec = sklearn.metrics.precision_score(y_test, y_pred)\n",
        "        rec = sklearn.metrics.recall_score(y_test, y_pred)\n",
        "        f1 = sklearn.metrics.f1_score(y_test, y_pred)\n",
        "        \n",
        "        return name, model, history, acc, f1, prec, rec, auc"
      ],
      "metadata": {
        "id": "9zQPOcdjNxDS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Runner:\n",
        "    def __init__(self):\n",
        "        self.scores_dict = {\n",
        "            \"MeasureName\": [],\n",
        "            \"Accuracy\": [],\n",
        "            \"F1\": [],\n",
        "            \"Precision\": [],\n",
        "            \"Recall\": [],\n",
        "            \"AUC\": [],\n",
        "        }\n",
        "        self.training_histories = dict()\n",
        "        self.scores_df = None\n",
        "\n",
        "    def run(self, name:str):\n",
        "        message = f\"Running : {name}\"\n",
        "        print(message)\n",
        "        print(\"-\" * len(message))\n",
        "\n",
        "        name, model, history, accuracy, f1, precision, recall, auc = Model.train_model(name)\n",
        "        self.training_histories[name] = history\n",
        "        self.scores_dict[\"MeasureName\"].append(name)\n",
        "        self.scores_dict[\"Accuracy\"].append(accuracy)\n",
        "        self.scores_dict[\"F1\"].append(f1)\n",
        "        self.scores_dict[\"Precision\"].append(precision)\n",
        "        self.scores_dict[\"Recall\"].append(recall)\n",
        "        self.scores_dict[\"AUC\"].append(auc)\n",
        "\n",
        "    def print(self):\n",
        "        df = pd.DataFrame(self.scores_dict)\n",
        "        print(df)\n",
        "    \n",
        "    def print_latex(self):\n",
        "        df = pd.DataFrame(self.scores_dict)\n",
        "        print(df.round(3).to_latex(index=False))\n",
        "\n",
        "    def pickle_histories(self, pickle_filename):\n",
        "        import pickle\n",
        "        with open(pickle_filename, \"wb\") as f:\n",
        "            pickle.dump(self.training_histories, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def save_result_df(self, csv_filename):\n",
        "        df = pd.DataFrame(self.scores_dict)\n",
        "        df.to_csv(csv_filename)"
      ],
      "metadata": {
        "id": "1bncaqmDOfKB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this if using NapierOne\n",
        "print(\"USING NAPIER ONE\")\n",
        "print(\"******************************************\")\n",
        "print()\n",
        "load_datasets.cache_clear()\n",
        "get_columns.cache_clear()\n",
        "get_train_test_df.cache_clear()\n",
        "\n",
        "INCLUDE_NAPIERONE = True\n",
        "runner = Runner()\n",
        "# Now run all the different combinations of features\n",
        "_ = [runner.run(str(c)) for c in get_columns().keys() if \"all_\" not in str(c).lower()]\n",
        "runner.pickle_histories(\"n1.history.pickle\")\n",
        "runner.save_result_df(\"n1.DenseAnnResult.csv\")\n",
        "!mkdir -p /content/drive/MyDrive/ProjSave\n",
        "!rm -f /content/drive/MyDrive/ProjSave/n1.history.pickle\n",
        "!rm -f /content/drive/MyDrive/ProjSave/n1.DenseAnnResult.csv\n",
        "!cp n1.history.pickle /content/drive/MyDrive/ProjSave\n",
        "!cp n1.DenseAnnResult.csv /content/drive/MyDrive/ProjSave\n",
        "!ls -l /content/drive/MyDrive/ProjSave\n",
        "\n",
        "using_napier_runner = runner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2bSigbpT_wf",
        "outputId": "95cec157-6e9a-46f8-8a35-cebbc136c5fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING NAPIER ONE\n",
            "******************************************\n",
            "\n",
            "Reloading Datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this if not using Napier One\n",
        "print(\"NOT USING NAPIER ONE\")\n",
        "print(\"******************************************\")\n",
        "print()\n",
        "load_datasets.cache_clear()\n",
        "get_columns.cache_clear()\n",
        "get_train_test_df.cache_clear()\n",
        "\n",
        "INCLUDE_NAPIERONE = False\n",
        "runner = Runner()\n",
        "# Now run all the different combinations of features\n",
        "_ = [runner.run(str(c)) for c in get_columns().keys() if \"all\" != str(c).lower()\n",
        "\n",
        "runner.pickle_histories(\"history.pickle\")\n",
        "runner.save_result_df(\"DenseAnnResult.csv\")\n",
        "!mkdir -p /content/drive/MyDrive/ProjSave\n",
        "!rm -f /content/drive/MyDrive/ProjSave/history.pickle\n",
        "!rm -f /content/drive/MyDrive/ProjSave/DenseAnnResult.csv\n",
        "!cp history.pickle /content/drive/MyDrive/ProjSave\n",
        "!cp DenseAnnResult.csv /content/drive/MyDrive/ProjSave\n",
        "!ls -l /content/drive/MyDrive/ProjSave\n",
        "\n",
        "not_using_napier_runner = runner"
      ],
      "metadata": {
        "id": "RCSsO8eqURZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runner = using_napier_runner\n",
        "print(\"USING NAPIER ONE\")\n",
        "print(\"******************************************\")\n",
        "runner.print()\n",
        "print()\n",
        "print(\"--------------------------------\")\n",
        "print()\n",
        "runner.print_latex()\n",
        "print()\n",
        "print(\"--------------------------------\")\n",
        "print()\n",
        "pd.DataFrame(runner.scores_dict)"
      ],
      "metadata": {
        "id": "BQ6tCaItXtxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runner = not_using_napier_runner\n",
        "print(\"NOT USING NAPIER ONE\")\n",
        "print(\"******************************************\")\n",
        "runner.print()\n",
        "print()\n",
        "print(\"--------------------------------\")\n",
        "print()\n",
        "runner.print_latex()\n",
        "print()\n",
        "print(\"--------------------------------\")\n",
        "print()\n",
        "pd.DataFrame(runner.scores_dict)"
      ],
      "metadata": {
        "id": "C4g6JWEGX6sk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}