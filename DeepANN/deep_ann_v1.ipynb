{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-ann-v1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPT8opz2qdASSpiBaGFA+0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/msc-ai-project/blob/main/DeepANN/deep_ann_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQP4gxMADtVr",
        "outputId": "ba0c60cb-d8f9-427b-cce0-b503960e9753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "USE_FLOAT32 = False\n",
        "REDUCE_DF_SIZE = True\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "!pip install tensorflow-addons\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging, sys, random, glob\n",
        "from google.colab import drive\n",
        "from functools import lru_cache\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from tensorflow import keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "import IPython\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import gc\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "def set_random_seed():\n",
        "    np.random.seed(1)\n",
        "    random.seed(1)\n",
        "    tf.random.set_seed(1)\n",
        "\n",
        "root = logging.getLogger()\n",
        "root.setLevel(logging.INFO)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setLevel(logging.DEBUG)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "root.addHandler(handler)\n",
        "\n",
        "!if [[ -d /content/drive/MyDrive/MSCPROJDATA && ! -d MSCPROJDATA ]]; then cp -r /content/drive/MyDrive/MSCPROJDATA .; fi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_names = [\"baseline\", \"advanced-only\", \"fourier-only\", \"baseline-and-fourier\", \"advanced-and-fourier\", \"advanced\", \"fourier\"]\n",
        "def get_columns_from_df(thisdf):\n",
        "    baseline_columns = [c for c in thisdf.columns if c.startswith('baseline') and \"head\" not in c and \"tail\" not in c]\n",
        "    baseline_columns = [c for c in baseline_columns if \"filesize\" not in c]\n",
        "    baseline_columns = [c for c in baseline_columns if \"begin\" not in c and \"end\" not in c]\n",
        "\n",
        "    advanced_columns = [c for c in thisdf.columns if \"advanced\" in c]\n",
        "    advanced_columns = [c for c in advanced_columns if \"begin\" not in c and \"end\" not in c]\n",
        "    advanced_columns = [c for c in advanced_columns if \"head\" not in c and \"tail\" not in c]\n",
        "    advanced_columns = [c for c in advanced_columns if \"start\" not in c]\n",
        "    advanced_columns_only = list(set(advanced_columns))\n",
        "    advanced_columns = list(set(advanced_columns + baseline_columns))\n",
        "\n",
        "    fourier_columns = [c for c in thisdf.columns if \"fourier\" in c and \"value\" not in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"1byte\" in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"begin\" not in c and \"end\" not in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"head\" not in c and \"tail\" not in c]\n",
        "    fourier_columns = [c for c in fourier_columns if \"start\" not in c]\n",
        "    fourier_columns_only = list(set(fourier_columns))\n",
        "    fourier_columns = list(set(advanced_columns + fourier_columns))\n",
        "    \n",
        "    baseline_and_advanced = list(set(baseline_columns + advanced_columns_only))\n",
        "    baseline_and_fourier = list(set(baseline_columns + fourier_columns_only))\n",
        "    advanced_and_fourier = list(set(advanced_columns_only + fourier_columns_only))\n",
        "\n",
        "    all_columns = [c for c in thisdf.columns]\n",
        "    \n",
        "    return {\\\n",
        "        \"baseline\": baseline_columns,\\\n",
        "        \"advanced-only\": advanced_columns_only,\\\n",
        "        \"fourier-only\": fourier_columns_only,\\\n",
        "        \"baseline-and-fourier\": baseline_and_fourier,\\\n",
        "        \"advanced-and-fourier\": advanced_and_fourier,\\\n",
        "        \"advanced\": advanced_columns,\\\n",
        "        \"fourier\": fourier_columns,\\\n",
        "        \"all_columns\": all_columns,\n",
        "    }\n",
        "\n",
        "\n",
        "def reduce_df_size(df):\n",
        "    original_usage = df.memory_usage().sum() / (1024**2)\n",
        "    for col in df.columns:\n",
        "        if \"fourier.value\" in col or \\\n",
        "            \"tail\" in col or \\\n",
        "            \"head\" in col or \\\n",
        "            \"end\" in col or \\\n",
        "            \"begin\" in col:\n",
        "            df.drop([col], axis=1, inplace=True)\n",
        "            continue\n",
        "        if USE_FLOAT32:\n",
        "            if \"float64\" in str(df[col].dtype):\n",
        "                df[col] = df[col].astype('float32')\n",
        "        elif \"int64\" in str(df[col].dtype):\n",
        "            df[col] = df[col].astype(\"int8\")\n",
        "    new_usage = df.memory_usage().sum() / (1024**2)\n",
        "    print(f\"Reduced memory sage from {original_usage} to {new_usage}\")\n",
        "    return df\n",
        "\n",
        "def call_gc():\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            gc.collect(j)\n",
        "\n",
        "@lru_cache(maxsize=5)\n",
        "def load_datasets():\n",
        "    datasets = dict()\n",
        "    for file in glob.glob(\"MSCPROJDATA/**.parquet.gz\", recursive=True):\n",
        "        if not file.startswith(\"MSCPROJDATA/n1\"):\n",
        "            print(f\"Loading {file}\")\n",
        "            df = pd.read_parquet(file)\n",
        "            datasets[file] = df\n",
        "            df[\"is_encrypted\"] = 1 if \"encr\" in file.lower() else 0\n",
        "    \n",
        "            if REDUCE_DF_SIZE:\n",
        "                df = reduce_df_size(df)\n",
        "\n",
        "            df = df.sample(frac=1).reset_index(drop=True)\n",
        "            datasets[file] = df\n",
        "            call_gc()\n",
        "\n",
        "    return datasets\n",
        "\n",
        "@lru_cache(maxsize=5)\n",
        "def get_columns():\n",
        "    datasets = load_datasets()\n",
        "    df = list(datasets.values())[0]\n",
        "    return get_columns_from_df(df)\n",
        "\n",
        "_ = load_datasets()\n",
        "_ = get_columns()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxRiyNG5EAiU",
        "outputId": "cad976c3-8462-4223-8735-0eb2e91ffc17"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MSCPROJDATA/plaintext.expanded.parquet.gz\n",
            "Reduced memory sage from 417.6138381958008 to 50.274044036865234\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v1.b32.parquet.gz\n",
            "Reduced memory sage from 417.6138381958008 to 47.575284004211426\n",
            "Loading MSCPROJDATA/plaintext.combined.parquet.gz\n",
            "Reduced memory sage from 417.6138381958008 to 50.274044036865234\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v1.parquet.gz\n",
            "Reduced memory sage from 417.6138381958008 to 49.19454002380371\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v2.parquet.gz\n",
            "Reduced memory sage from 417.6138381958008 to 48.65478801727295\n",
            "Loading MSCPROJDATA/expanded.plaintext.base32.parquet.gz\n",
            "Reduced memory sage from 417.6138381958008 to 48.65478801727295\n",
            "Loading MSCPROJDATA/plaintext.base32.combined.parquet.gz\n",
            "Reduced memory sage from 417.6138381958008 to 48.65478801727295\n",
            "Loading MSCPROJDATA/expanded.pyencrypted_v2.base32.parquet.gz\n",
            "Reduced memory sage from 417.6138381958008 to 48.65478801727295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(maxsize=5)\n",
        "def get_train_test_df():\n",
        "    datasets = load_datasets()\n",
        "    df = pd.concat([v for v in datasets.values()])\n",
        "    x_cols = [str(c) for c in df.columns if \"is_encrypted\" != str(c)]\n",
        "    y_cols = \"is_encrypted\"\n",
        "\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    df.fillna(0.0)\n",
        "\n",
        "    X = df[x_cols]\n",
        "    y = df[y_cols]\n",
        "\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    X = X - X.min()\n",
        "    X = X / X.max()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "_ = get_train_test_df()"
      ],
      "metadata": {
        "id": "pMTF4z0nRNxM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    @staticmethod\n",
        "    def print_and_plot(model):\n",
        "        print(model.summary())\n",
        "        plot = tf.keras.utils.plot_model(\n",
        "            model,\n",
        "            show_shapes=True,\n",
        "            expand_nested=True)\n",
        "        IPython.display.display(plot)\n",
        "\n",
        "    @staticmethod\n",
        "    def create(name:str, columns:list):\n",
        "        X_train, X_test, y_train, y_test = get_train_test_df()\n",
        "        X_train = X_train[columns].to_numpy()\n",
        "\n",
        "        model = tf.keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(8, input_dim=X_train.shape[1], activation='relu', name=\"Dense-8\"),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(4, activation = 'relu', name=\"Dense-4\"),\n",
        "                layers.Dropout(0.2),\n",
        "                layers.Dense(2, activation = 'relu', name=\"Dense-2\"),\n",
        "                layers.Dense(1, activation = 'sigmoid')\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        model.compile(\\\n",
        "            loss = 'binary_crossentropy',\n",
        "            optimizer = 'adam',\\\n",
        "            metrics = [\n",
        "                tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "                tf.keras.metrics.Precision(name='precision'),\n",
        "                tf.keras.metrics.Recall(name='recall'),\n",
        "                tfa.metrics.F1Score(num_classes=1, name='f1_score'),\n",
        "            ])\n",
        "            #callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")])\n",
        "        \n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def train_model(name):\n",
        "        X_train, X_test, y_train, y_test = get_train_test_df()\n",
        "\n",
        "        columns = get_columns()\n",
        "        columns = columns[name]\n",
        "\n",
        "        X_train = X_train[columns]\n",
        "        X_test = X_test[columns]\n",
        "\n",
        "        n_cross_val = X_train.shape[0]\n",
        "        n_train = n_cross_val - n_cross_val // 5\n",
        "        n_cross_val = n_cross_val // 5\n",
        "\n",
        "        X_cross_val = X_train.tail(n_cross_val)\n",
        "        y_cross_val = y_train[-n_cross_val:]\n",
        "\n",
        "        X_train = X_train.head(n_train)\n",
        "        y_train = y_train[:n_train]\n",
        "\n",
        "        es_clbk = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
        "\n",
        "        model = Model.create(name, columns)\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            epochs=NUM_EPOCHS,\n",
        "                            validation_data=(X_cross_val, y_cross_val),\n",
        "                            callbacks=[es_clbk],\n",
        "                            batch_size=BATCH_SIZE)\n",
        "        \n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred = y_pred > 0.5\n",
        "\n",
        "        acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
        "        prec = sklearn.metrics.precision_score(y_test, y_pred)\n",
        "        rec = sklearn.metrics.recall_score(y_test, y_pred)\n",
        "        f1 = sklearn.metrics.f1_score(y_test, y_pred)\n",
        "        \n",
        "        return name, model, history, acc, f1, prec, rec"
      ],
      "metadata": {
        "id": "9zQPOcdjNxDS"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[k for k in columns.keys()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX5HQN0vXzC7",
        "outputId": "21ae640f-e6e2-493e-b791-a88e1644e4a5"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['baseline',\n",
              " 'advanced-only',\n",
              " 'fourier-only',\n",
              " 'baseline-and-fourier',\n",
              " 'advanced-and-fourier',\n",
              " 'advanced',\n",
              " 'fourier',\n",
              " 'all_columns']"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name, model, history, acc, f1, prec, rec = Model.train_model(\"baseline\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8FSRCqpQ0Aj",
        "outputId": "99a807a3-0a06-4623-c01e-6d4d5c428a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            " 1573/12937 [==>...........................] - ETA: 44s - loss: 0.6847 - accuracy: 0.5387 - precision: 0.5411 - recall: 0.5235 - f1_score: 0.6677"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc, f1, prec, rec)"
      ],
      "metadata": {
        "id": "spXq2MEEZC5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p8wUUEZSgPu5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}