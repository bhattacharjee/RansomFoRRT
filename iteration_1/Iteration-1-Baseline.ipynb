{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea20739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f716a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCRYPTED_DIRECTORY = \"data/govdocs_encrypted\"\n",
    "PLAINTEXT_DIRECTORY = \"data/govdocs_plaintext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1205a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/govdocs_plaintext/777.parquet.gzip\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z1/hfz3q2b52bx3hb8nh0z1q89c0000gp/T/ipykernel_16818/4165478399.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_plaintext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPLAINTEXT_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf_plaintext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_encrypted\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf_encrypted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENCRYPTED_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/z1/hfz3q2b52bx3hb8nh0z1q89c0000gp/T/ipykernel_16818/4165478399.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{directory}/**.parquet.gzip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mdataframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \"\"\"\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     return impl.read(\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n - \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;34m\"Unable to find a usable engine; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;34m\"tried using: 'pyarrow', 'fastparquet'.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "def load_data(directory):\n",
    "    dataframes = list()\n",
    "    for f in glob.glob(f\"{directory}/**.parquet.gzip\"):\n",
    "        print(f)\n",
    "        df = pd.read_parquet(f)\n",
    "        dataframes.append(df)\n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "df_plaintext = load_data(PLAINTEXT_DIRECTORY)\n",
    "df_plaintext[\"is_encrypted\"] = 0\n",
    "df_encrypted = load_data(ENCRYPTED_DIRECTORY)\n",
    "df_encrypted[\"is_encrypted\"] = 1\n",
    "\n",
    "master_df = pd.concat([df_plaintext, df_encrypted])\n",
    "\n",
    "# Filter out only webp\n",
    "# master_df = master_df[master_df['extended.extension'] != '.webp']\n",
    "print(\"only webp: \", master_df.shape)\n",
    "\n",
    "# Filter out files which are larger than 4k\n",
    "#master_df = master_df[master_df['baseline.filesize'] > 4096]\n",
    "print(\"greater than 4096: \", master_df.shape)\n",
    "\n",
    "# Shuffle\n",
    "master_df = master_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# fill Nan with 0\n",
    "master_df['baseline.chisquare_end'] = master_df['baseline.chisquare_end'].fillna(0.0)\n",
    "master_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5952bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns_to_consider = [c for c in master_df.columns]\n",
    "#columns_to_consider = [c for c in master_df.columns if c.startswith(\"baseline\")]\n",
    "#columns_to_consider.append('is_encrypted')\n",
    "columns_to_consider = [c for c in master_df.columns if c != 'extended.extension']\n",
    "\n",
    "\n",
    "interesting_df = master_df[columns_to_consider]\n",
    "\n",
    "X = interesting_df[[c for c in interesting_df.columns if c.startswith('baseline')]]\n",
    "X = interesting_df[[c for c in interesting_df.columns if c != 'is_encrypted']]\n",
    "X = X[[c for c in X.columns if c != 'is_encrypted']]\n",
    "\n",
    "# get rid of tail and head metrics\n",
    "X = X[[c for c in X.columns if \"begin\" not in c and \"head\" not in c]]\n",
    "X = X[[c for c in X.columns if \"tail\" not in c and \"end\" not in c]]\n",
    "\n",
    "X = X[[c for c in X.columns if \"kurtosis\" not in c and \"skew\" not in c]]\n",
    "\n",
    "y = interesting_df['is_encrypted']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "estimators = [('std,', MinMaxScaler()), ('LogisticRegressor', LogisticRegression())]\n",
    "estimators = [('std,', MinMaxScaler()), ('RFC', rfc)]\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred)\n",
    "print(precision, recall, fscore, support)\n",
    "print(f\"F1 = {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = master_df[X_train.isna().any(axis=1)]\n",
    "#print(df1)\n",
    "#print(X_train.describe().T)\n",
    "print(rfc.feature_importances_)\n",
    "plt.barh(X_train.columns, rfc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4decae59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/phantom/dev/msc-ai/project/msc-ai-project/iteration_1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee881566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
